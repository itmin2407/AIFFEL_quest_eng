{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38219670-cf2d-4fad-9e28-a97665f1baa6",
   "metadata": {},
   "source": [
    "### LSTM Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad57646f-be85-45fb-b294-13698e75c55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝~\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        print(\"입력 Shape:\", src.size())\n",
    "\n",
    "        embedded = self.embedding(src)\n",
    "        print(\"Embedding Layer를 거친 Shape:\", embedded.size())\n",
    "\n",
    "        outputs, (h_0, c_0) = self.rnn(embedded)\n",
    "        print(\"LSTM Layer의 Output Shape:\", outputs.size())\n",
    "        print(\"LSTM Layer의 Hidden State Shape:\", h_0.size())\n",
    "        print(\"LSTM Layer의 Cell State Shape:\", c_0.size())\n",
    "\n",
    "        return outputs, h_0, c_0\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e680796-c456-4817-8701-f07450e6b2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 30000\n",
      "Embedidng Size: 256\n",
      "LSTM Size: 512\n",
      "Batch Size: 1\n",
      "Sample Sequence Length: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "emb_size = 256\n",
    "lstm_size = 512\n",
    "batch_size = 1\n",
    "sample_seq_len = 3\n",
    "\n",
    "print(\"Vocab Size: {0}\".format(vocab_size))\n",
    "print(\"Embedidng Size: {0}\".format(emb_size))\n",
    "print(\"LSTM Size: {0}\".format(lstm_size))\n",
    "print(\"Batch Size: {0}\".format(batch_size))\n",
    "print(\"Sample Sequence Length: {0}\\n\".format(sample_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0ddffe-b1e4-4d9b-9d45-33ce2db25ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 Shape: torch.Size([1, 3])\n",
      "Embedding Layer를 거친 Shape: torch.Size([1, 3, 256])\n",
      "LSTM Layer의 Output Shape: torch.Size([1, 3, 512])\n",
      "LSTM Layer의 Hidden State Shape: torch.Size([1, 1, 512])\n",
      "LSTM Layer의 Cell State Shape: torch.Size([1, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "encoder = Encoder(vocab_size, emb_size, lstm_size)\n",
    "sample_input = torch.randint(0, vocab_size, (batch_size, sample_seq_len))\n",
    "\n",
    "sample_output, hidden, cell = encoder(sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeb744e-57c5-444f-b422-fb2debb529b7",
   "metadata": {},
   "source": [
    "### LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80310672-74d7-42c2-a035-33da3e4d806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim + hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell, context):\n",
    "        print(\"입력 Shape:\", x.size())\n",
    "\n",
    "        embedded = self.embedding(x)\n",
    "        print(\"Embedding Layer를 거친 Shape:\", embedded.size())\n",
    "\n",
    "        embedded = torch.cat((embedded, context), dim=2)\n",
    "        print(\"Context Vector가 더해진 Shape:\", embedded.size())\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        print(\"LSTM Layer의 Output Shape:\", output.size())\n",
    "\n",
    "        output = self.fc(output)\n",
    "        print(\"Decoder 최종 Output Shape:\", output.size())\n",
    "\n",
    "        return output, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "372f2a4b-ed4c-4d20-86c7-2c0247f04d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 30000\n",
      "Embedidng Size: 256\n",
      "LSTM Size: 512\n",
      "Batch Size: 1\n",
      "Sample Sequence Length: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab Size: {0}\".format(vocab_size))\n",
    "print(\"Embedidng Size: {0}\".format(emb_size))\n",
    "print(\"LSTM Size: {0}\".format(lstm_size))\n",
    "print(\"Batch Size: {0}\".format(batch_size))\n",
    "print(\"Sample Sequence Length: {0}\\n\".format(sample_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eece4040-3056-432f-851c-d94342f1d3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 Shape: torch.Size([1, 3])\n",
      "Embedding Layer를 거친 Shape: torch.Size([1, 3, 256])\n",
      "Context Vector가 더해진 Shape: torch.Size([1, 3, 768])\n",
      "LSTM Layer의 Output Shape: torch.Size([1, 3, 512])\n",
      "Decoder 최종 Output Shape: torch.Size([1, 3, 30000])\n"
     ]
    }
   ],
   "source": [
    "decoder_input = torch.randint(0, vocab_size, (batch_size, sample_seq_len))  # (batch_size, seq_length)\n",
    "\n",
    "decoder = Decoder(vocab_size, emb_size, lstm_size)\n",
    "\n",
    "dec_output, hidden, cell = decoder(decoder_input, hidden, cell, sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd7f50-751d-484f-9ab3-d4b1001b7b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9353efcb-254a-4512-b4a3-141dfde31e3e",
   "metadata": {},
   "source": [
    "### Bahdanau Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "595e835d-b118-492b-9dee-756826a8d7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden State를 100차원으로 Mapping\n",
      "\n",
      "[ H_encoder ] Shape: torch.Size([1, 10, 512])\n",
      "[ W_encoder X H_encoder ] Shape: torch.Size([1, 10, 100])\n",
      "\n",
      "[ H_decoder ] Shape: torch.Size([1, 512])\n",
      "[ W_decoder X H_decoder ] Shape: torch.Size([1, 1, 100])\n",
      "[ Score_alignment ] Shape: torch.Size([1, 10, 1])\n",
      "\n",
      "최종 Weight:\n",
      " [[0.09967567 0.11247154 0.10304306 0.09597894 0.09203966 0.10856623\n",
      "  0.09853788 0.09047566 0.10224074 0.0969707 ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W_decoder = nn.Linear(512, units)  # Decoder hidden state -> units\n",
    "        self.W_encoder = nn.Linear(512, units)  # Encoder hidden state -> units\n",
    "        self.W_combine = nn.Linear(units, 1)   # Alignment score -> scalar weight\n",
    "\n",
    "    def forward(self, H_encoder, H_decoder):\n",
    "        print(\"[ H_encoder ] Shape:\", H_encoder.shape)  # (batch, seq_len, hidden_dim)\n",
    "\n",
    "        H_encoder = self.W_encoder(H_encoder)\n",
    "        print(\"[ W_encoder X H_encoder ] Shape:\", H_encoder.shape)  # (batch, seq_len, units)\n",
    "\n",
    "        print(\"\\n[ H_decoder ] Shape:\", H_decoder.shape)  # (batch, hidden_dim)\n",
    "        H_decoder = H_decoder.unsqueeze(1)  # (batch, 1, hidden_dim)\n",
    "        H_decoder = self.W_decoder(H_decoder)  # (batch, 1, units)\n",
    "\n",
    "        print(\"[ W_decoder X H_decoder ] Shape:\", H_decoder.shape)  # (batch, 1, units)\n",
    "\n",
    "        score = self.W_combine(torch.tanh(H_decoder + H_encoder))  # (batch, seq_len, 1)\n",
    "        print(\"[ Score_alignment ] Shape:\", score.shape)\n",
    "\n",
    "        attention_weights = F.softmax(score, dim=1)  # (batch, seq_len, 1)\n",
    "        print(\"\\n최종 Weight:\\n\", attention_weights.squeeze(-1).detach().numpy())\n",
    "\n",
    "        context_vector = attention_weights * H_decoder  # (batch, seq_len, units)\n",
    "        context_vector = torch.sum(context_vector, dim=1)  # (batch, units)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# 설정\n",
    "W_size = 100\n",
    "print(f\"Hidden State를 {W_size}차원으로 Mapping\\n\")\n",
    "\n",
    "# 모델 생성\n",
    "attention = BahdanauAttention(W_size)\n",
    "\n",
    "# 입력 데이터 (배치 크기 = 1)\n",
    "enc_state = torch.rand((1, 10, 512))  # (batch, seq_len, hidden_dim)\n",
    "dec_state = torch.rand((1, 512))  # (batch, hidden_dim)\n",
    "\n",
    "# 실행\n",
    "_ = attention(enc_state, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab9104-e511-48be-8391-8c32ed00d5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd740818-4723-48d3-99bd-dcebc615bb9c",
   "metadata": {},
   "source": [
    "### Luong Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "183278be-b26a-428c-a70a-a9e7195ec8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ H_encoder ] Shape: torch.Size([1, 10, 512])\n",
      "[ W_encoder X H_encoder ] Shape: torch.Size([1, 10, 512])\n",
      "[ Score_alignment ] Shape: torch.Size([1, 10, 1])\n",
      "\n",
      "최종 Weight:\n",
      " [[7.6420554e-03 6.3642205e-05 1.1277688e-01 4.4557350e-03 5.4476041e-02\n",
      "  5.1700205e-01 7.5882631e-03 2.1896820e-01 1.6297560e-02 6.0729597e-02]]\n"
     ]
    }
   ],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.W_combine = nn.Linear(units, units)  # Encoder hidden state 변환\n",
    "\n",
    "    def forward(self, H_encoder, H_decoder):\n",
    "        print(\"[ H_encoder ] Shape:\", H_encoder.shape)  # (batch, seq_len, hidden_dim)\n",
    "\n",
    "        WH = self.W_combine(H_encoder)  # (batch, seq_len, hidden_dim)\n",
    "        print(\"[ W_encoder X H_encoder ] Shape:\", WH.shape)\n",
    "\n",
    "        H_decoder = H_decoder.unsqueeze(1)  # (batch, 1, hidden_dim)\n",
    "        alignment = torch.bmm(WH, H_decoder.transpose(1, 2))  # (batch, seq_len, 1)\n",
    "        print(\"[ Score_alignment ] Shape:\", alignment.shape)\n",
    "\n",
    "        attention_weights = F.softmax(alignment, dim=1)  # (batch, seq_len, 1)\n",
    "        print(\"\\n최종 Weight:\\n\", attention_weights.squeeze(-1).detach().numpy())\n",
    "\n",
    "        attention_weights = attention_weights.squeeze(-1)  # (batch, seq_len)\n",
    "        context_vector = torch.bmm(attention_weights.unsqueeze(1), H_encoder)  # (batch, 1, hidden_dim)\n",
    "        context_vector = context_vector.squeeze(1)  # (batch, hidden_dim)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# 설정\n",
    "emb_dim = 512\n",
    "attention = LuongAttention(emb_dim)\n",
    "\n",
    "# 입력 데이터 (배치 크기 = 1)\n",
    "enc_state = torch.rand((1, 10, emb_dim))  # (batch, seq_len, hidden_dim)\n",
    "dec_state = torch.rand((1, emb_dim))  # (batch, hidden_dim)\n",
    "\n",
    "# 실행\n",
    "_ = attention(enc_state, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37d14c-7f30-4efb-b6cd-085a512027b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
