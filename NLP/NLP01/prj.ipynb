{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00bd0624-16c5-4c32-930c-8181b7545750",
   "metadata": {},
   "source": [
    "# Seq2se으로 번역기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2155c9-f74c-47f6-8857-8a9e3a988003",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 GitHub 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e5373cf-e3f2-41ac-8a61-e6d31c949ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import tarfile \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ae46717-ba02-4306-9add-9c57d577cd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b910b6b1-cc2e-4e85-a118-062f162a8723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract root already exists: data_korean_english_news_v1/korean-parallel-corpora-master\n",
      "NEWS_DIR: data_korean_english_news_v1/korean-parallel-corpora-master/korean-english-news-v1\n",
      "FILES before tar extraction: ['korean-english-park.train.tar.gz', 'korean-english-park.test.ko', 'korean-english-park.test.tar.gz', 'korean-english-park.dev.tar.gz', 'korean-english-park.test.en', 'korean-english-park.dev.en', 'README.md', 'korean-english-park.train.ko', 'korean-english-park.train.en', 'korean-english-park.dev.ko']\n",
      "Extracting: data_korean_english_news_v1/korean-parallel-corpora-master/korean-english-news-v1/korean-english-park.train.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_845/822202083.py:37: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(NEWS_DIR)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: korean-english-park.train.tar.gz\n",
      "Extracting: data_korean_english_news_v1/korean-parallel-corpora-master/korean-english-news-v1/korean-english-park.test.tar.gz\n",
      "Done: korean-english-park.test.tar.gz\n",
      "Extracting: data_korean_english_news_v1/korean-parallel-corpora-master/korean-english-news-v1/korean-english-park.dev.tar.gz\n",
      "Done: korean-english-park.dev.tar.gz\n",
      "FILES after tar extraction: ['korean-english-park.train.tar.gz', 'korean-english-park.test.ko', 'korean-english-park.test.tar.gz', 'korean-english-park.dev.tar.gz', 'korean-english-park.test.en', 'korean-english-park.dev.en', 'README.md', 'korean-english-park.train.ko', 'korean-english-park.train.en', 'korean-english-park.dev.ko']\n"
     ]
    }
   ],
   "source": [
    "# 1) 데이터 폴더\n",
    "DATA_DIR = \"data_korean_english_news_v1\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# 2) 저장소 zip URL\n",
    "ZIP_URL = \"https://github.com/jungyeul/korean-parallel-corpora/archive/refs/heads/master.zip\"  # [web:1]\n",
    "ZIP_PATH = os.path.join(DATA_DIR, \"korean-parallel-corpora-master.zip\")\n",
    "\n",
    "# 3) 다운로드\n",
    "if not os.path.exists(ZIP_PATH):\n",
    "    print(\"Downloading corpus zip...\")\n",
    "    urllib.request.urlretrieve(ZIP_URL, ZIP_PATH)\n",
    "    print(\"Download finished.\")\n",
    "\n",
    "# 4) zip 압축 해제 (저장소 디렉토리 생성)\n",
    "EXTRACT_ROOT = os.path.join(DATA_DIR, \"korean-parallel-corpora-master\")\n",
    "if not os.path.exists(EXTRACT_ROOT):\n",
    "    print(\"Extracting zip...\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
    "        zf.extractall(DATA_DIR)\n",
    "    print(\"Extraction finished.\")\n",
    "else:\n",
    "    print(\"Extract root already exists:\", EXTRACT_ROOT)\n",
    "\n",
    "# 5) korean-english-news-v1 디렉토리\n",
    "BASE_DIR = EXTRACT_ROOT  # = data_korean_english_news_v1/korean-parallel-corpora-master\n",
    "NEWS_DIR = os.path.join(BASE_DIR, \"korean-english-news-v1\")\n",
    "print(\"NEWS_DIR:\", NEWS_DIR)\n",
    "print(\"FILES before tar extraction:\", os.listdir(NEWS_DIR))\n",
    "\n",
    "# 6) korean-english-park.*.tar.gz 모두 압축 해제\n",
    "for fname in os.listdir(NEWS_DIR):\n",
    "    if fname.endswith(\".tar.gz\"):\n",
    "        tar_path = os.path.join(NEWS_DIR, fname)\n",
    "        print(\"Extracting:\", tar_path)\n",
    "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "            tar.extractall(NEWS_DIR)\n",
    "        print(\"Done:\", fname)\n",
    "\n",
    "print(\"FILES after tar extraction:\", os.listdir(NEWS_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b090cd68-1d50-4aab-a6b6-a37eae49f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BASE_PATH = \"korean-english-park.train\"\n",
    "KO_TRAIN_PATH = os.path.join(NEWS_DIR, \"korean-english-park.train.ko\")\n",
    "EN_TRAIN_PATH = os.path.join(NEWS_DIR, \"korean-english-park.train.en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cfc2c6-5cdc-4999-a249-2e1b32afc95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14f96121-7dab-4f4d-85e5-d99594ee9272",
   "metadata": {},
   "source": [
    "## 2. 전처리 및 토큰화\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b74fe41-a8ce-4e9d-961f-0f8bc7408617",
   "metadata": {},
   "source": [
    "### 2-1. 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5f5b371-6e1e-4d7f-af83-428e2b818c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 40\n",
    "MIN_FREQ = 2\n",
    "MAX_VOCAB = 20000\n",
    "\n",
    "def preprocess_en(text: str) -> str:\n",
    "    # 소문자화 + 알파벳/숫자/구두점만 남기기\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9,.!?\\' ]\", \" \", text)\n",
    "    # 구두점 앞뒤 공백 정리[web:18]\n",
    "    text = re.sub(r\"([,.!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_ko(text: str) -> str:\n",
    "    # 한글/숫자/기본 구두점만 남기기\n",
    "    text = re.sub(r\"[^가-힣0-9,.!?\\' ]\", \" \", text)\n",
    "    text = re.sub(r\"([,.!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_ko(text: str):\n",
    "    # 한국어: 어절 단위 토큰화 (공백 기준)[web:38]\n",
    "    return text.split()\n",
    "\n",
    "def tokenize_en(text: str):\n",
    "    # 영어: 공백 기준 토큰화\n",
    "    return text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03dae54-fb40-4efb-acc9-6efd0b27566c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd32dacc-ef12-48fd-a5c4-fc96798b4964",
   "metadata": {},
   "source": [
    "### 2-2. 파일 읽기 + 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b651b69a-e089-4d7a-8a12-2e00f75316b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 문장 쌍 수: 78941\n"
     ]
    }
   ],
   "source": [
    "with open(KO_TRAIN_PATH, encoding=\"utf-8\") as f_ko, \\\n",
    "     open(EN_TRAIN_PATH, encoding=\"utf-8\") as f_en:\n",
    "    kor_lines = [l.strip() for l in f_ko]\n",
    "    eng_lines = [l.strip() for l in f_en]\n",
    "\n",
    "assert len(kor_lines) == len(eng_lines)\n",
    "pairs = list(set(zip(kor_lines, eng_lines)))  # 중복 제거\n",
    "print(\"총 문장 쌍 수:\", len(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f6e5cb7-e9e1-414b-b695-e9f27eb55556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 문장 쌍 수: 71902\n",
      "예시 한국어 토큰: ['아일랜드만', '유일하게', '아일랜드', '헌법의', '필요', '조건들로', '인해', '국민투표를', '실시할', '예정이다']\n",
      "예시 영어 토큰: ['ireland', 'is', 'expected', 'to', 'hold', 'a', 'vote', 'on', 'the', 'treaty']\n"
     ]
    }
   ],
   "source": [
    "kor_tokenized = []\n",
    "eng_tokenized = []\n",
    "\n",
    "for ko, en in pairs:\n",
    "    ko_p = preprocess_ko(ko)\n",
    "    en_p = preprocess_en(en)\n",
    "    if not ko_p or not en_p:\n",
    "        continue\n",
    "\n",
    "    ko_tokens = tokenize_ko(ko_p)\n",
    "    en_tokens = tokenize_en(en_p)\n",
    "\n",
    "    if len(ko_tokens) <= MAX_LEN and len(en_tokens) <= MAX_LEN:\n",
    "        kor_tokenized.append(ko_tokens)\n",
    "        eng_tokenized.append(en_tokens)\n",
    "\n",
    "print(\"전처리 후 문장 쌍 수:\", len(kor_tokenized))\n",
    "print(\"예시 한국어 토큰:\", kor_tokenized[0][:10])\n",
    "print(\"예시 영어 토큰:\", eng_tokenized[0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0018b5-edd3-443f-a383-48f841178029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cf93e1a-8edc-4303-a3fb-f239d61c5530",
   "metadata": {},
   "source": [
    "## 3. Vocab, Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f3bc894-0f66-4c84-9fdb-1cadaa1d6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "\n",
    "def build_vocab(corpus, min_freq=2, max_vocab=20000):\n",
    "    counter = Counter(token for sent in corpus for token in sent)\n",
    "    vocab = SPECIAL_TOKENS.copy()\n",
    "    vocab += [w for w, c in counter.most_common(max_vocab) if c >= min_freq]\n",
    "    stoi = {w: i for i, w in enumerate(vocab)}\n",
    "    itos = {i: w for w, i in stoi.items()}\n",
    "    return stoi, itos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa2612c1-6684-44c6-ac00-3a6deb04c121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC vocab size: 20004\n",
      "TGT vocab size: 20004\n"
     ]
    }
   ],
   "source": [
    "src_stoi, src_itos = build_vocab(kor_tokenized, MIN_FREQ, MAX_VOCAB)\n",
    "tgt_stoi, tgt_itos = build_vocab(eng_tokenized, MIN_FREQ, MAX_VOCAB)\n",
    "\n",
    "PAD_IDX_SRC = src_stoi[\"<pad>\"]\n",
    "PAD_IDX_TGT = tgt_stoi[\"<pad>\"]\n",
    "SOS_IDX_TGT = tgt_stoi[\"<sos>\"]\n",
    "EOS_IDX_TGT = tgt_stoi[\"<eos>\"]\n",
    "\n",
    "print(\"SRC vocab size:\", len(src_stoi))\n",
    "print(\"TGT vocab size:\", len(tgt_stoi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c76938ac-8249-4a0a-8c71-fe244e44035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokens, stoi):\n",
    "    ids = [stoi.get(t, stoi[\"<unk>\"]) for t in tokens]\n",
    "    return [stoi[\"<sos>\"]] + ids + [stoi[\"<eos>\"]]\n",
    "\n",
    "def pad_sequence(seq, max_len, pad_idx):\n",
    "    return seq + [pad_idx] * (max_len - len(seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "16efed7c-634f-4c95-87e5-f308225a5cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_corpus, tgt_corpus, src_stoi, tgt_stoi, max_len=MAX_LEN+2):\n",
    "        self.src = src_corpus\n",
    "        self.tgt = tgt_corpus\n",
    "        self.src_stoi = src_stoi\n",
    "        self.tgt_stoi = tgt_stoi\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_ids = encode(self.src[idx], self.src_stoi)\n",
    "        tgt_ids = encode(self.tgt[idx], self.tgt_stoi)\n",
    "\n",
    "        src_ids = src_ids[:self.max_len]\n",
    "        tgt_ids = tgt_ids[:self.max_len]\n",
    "\n",
    "        src_padded = pad_sequence(src_ids, self.max_len, PAD_IDX_SRC)\n",
    "        tgt_padded = pad_sequence(tgt_ids, self.max_len, PAD_IDX_TGT)\n",
    "\n",
    "        return torch.tensor(src_padded), torch.tensor(tgt_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cafe6470-3b4c-4ad2-b711-18e97bfdb7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(kor_tokenized, eng_tokenized, src_stoi, tgt_stoi)\n",
    "BATCH_SIZE = 64\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6859a3-24f3-4354-a696-a7e7e58a8964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e376e60-0af6-41c1-af72-38c8dc8aa72e",
   "metadata": {},
   "source": [
    "## 4. Attentional Seq2Seq 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e8e9744-e816-41be-ad21-0a2d6f4131ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)               # (B, S, E)\n",
    "        outputs, hidden = self.gru(embedded)         # outputs: (B, S, H)\n",
    "        return outputs, hidden                       # hidden: (1, B, H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dec1b20e-9eda-4873-aa24-8c28ea7b69c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hid_dim, hid_dim, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        hidden = hidden[-1]                          # (B, H)\n",
    "        scores = torch.bmm(\n",
    "            encoder_outputs,                         # (B, S, H)\n",
    "            self.linear(hidden).unsqueeze(2)         # (B, H, 1)\n",
    "        ).squeeze(2)                                 # (B, S)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_weights = torch.softmax(scores, dim=1)  # (B, S)\n",
    "        context = torch.bmm(\n",
    "            attn_weights.unsqueeze(1),               # (B, 1, S)\n",
    "            encoder_outputs                          # (B, S, H)\n",
    "        ).squeeze(1)                                 # (B, H)\n",
    "        return context, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be20874e-831d-4bbe-96bf-499b9647b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(emb_dim + hid_dim, hid_dim, batch_first=True)\n",
    "        self.attn = LuongAttention(hid_dim)\n",
    "        self.fc_out = nn.Linear(hid_dim * 2, vocab_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs, mask=None):\n",
    "        embedded = self.embedding(input).unsqueeze(1)   # (B,1,E)\n",
    "        context, attn = self.attn(hidden, encoder_outputs, mask)  # (B,H)\n",
    "        context = context.unsqueeze(1)                  # (B,1,H)\n",
    "\n",
    "        rnn_input = torch.cat([embedded, context], dim=2)  # (B,1,E+H)\n",
    "        output, hidden = self.gru(rnn_input, hidden)       # output: (B,1,H)\n",
    "\n",
    "        output = output.squeeze(1)                     # (B,H)\n",
    "        context = context.squeeze(1)                   # (B,H)\n",
    "        logits = self.fc_out(torch.cat([output, context], dim=1))  # (B,V)\n",
    "        return logits, hidden, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9894a020-0a8c-4363-bdea-55694a4735d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def create_mask(self, src):\n",
    "        return (src != PAD_IDX_SRC).to(self.device)   # (B,S)\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_len, vocab_size, device=self.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        mask = self.create_mask(src)\n",
    "\n",
    "        input_tok = tgt[:, 0]  # <sos>\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            logits, hidden, attn = self.decoder(input_tok, hidden, encoder_outputs, mask)\n",
    "            outputs[:, t, :] = logits\n",
    "\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = logits.argmax(1)\n",
    "            input_tok = tgt[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f5c432c-1e8d-460f-a553-d391e8d2b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "\n",
    "encoder = Encoder(len(src_stoi), EMB_DIM, HID_DIM, PAD_IDX_SRC)\n",
    "decoder = Decoder(len(tgt_stoi), EMB_DIM, HID_DIM, PAD_IDX_TGT)\n",
    "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX_TGT)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57ca0d-baa5-4bd7-b472-9363e4ae7ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "056c8bce-c8a3-496d-9c78-759389471b32",
   "metadata": {},
   "source": [
    "## 5. 학습 루프 (loss 감소 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "be74d52d-0f66-4374-88f3-182c561fb6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(src, tgt, teacher_forcing_ratio=0.5)  # (B,T,V)\n",
    "\n",
    "        output_dim = outputs.shape[-1]\n",
    "        loss = criterion(\n",
    "            outputs[:, 1:].reshape(-1, output_dim),\n",
    "            tgt[:, 1:].reshape(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d5faead-1fab-4e14-a822-2178b7ed4716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 6.1644\n",
      "[Epoch 2] Train Loss: 5.3815\n",
      "[Epoch 3] Train Loss: 4.8171\n",
      "[Epoch 4] Train Loss: 4.3589\n",
      "[Epoch 5] Train Loss: 4.0676\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5  # 필요시 늘리기\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(model, dataloader, optimizer, criterion, DEVICE)\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c038ae45-f51d-4895-8762-cfa98b629d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "883c6337-ecff-4dbd-b229-bf5bfb431f93",
   "metadata": {},
   "source": [
    "## 6. 테스트용 디코더 / 번역 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2b906fe-3bf9-47d3-a02d-6b3a1efcdd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src_tokens, max_len=MAX_LEN+2):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src_ids = encode(src_tokens, src_stoi)\n",
    "        src_ids = src_ids[:max_len]\n",
    "        src_padded = pad_sequence(src_ids, max_len, PAD_IDX_SRC)\n",
    "        src_tensor = torch.tensor(src_padded).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "        mask = model.create_mask(src_tensor)\n",
    "\n",
    "        input_tok = torch.tensor([SOS_IDX_TGT], device=DEVICE)\n",
    "        generated = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            logits, hidden, attn = model.decoder(input_tok, hidden, encoder_outputs, mask)\n",
    "            top1 = logits.argmax(1)\n",
    "            if top1.item() == EOS_IDX_TGT:\n",
    "                break\n",
    "            generated.append(top1.item())\n",
    "            input_tok = top1\n",
    "\n",
    "    tokens = [tgt_itos.get(i, \"<unk>\") for i in generated]\n",
    "    tokens = [t for t in tokens if t not in [\"<pad>\", \"<sos>\", \"<eos>\"]]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b3a18659-478c-413c-a806-9be329f387ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_ko2en(model, sentence_ko: str):\n",
    "    ko_p = preprocess_ko(sentence_ko)\n",
    "    ko_tokens = tokenize_ko(ko_p)\n",
    "    return greedy_decode(model, ko_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d6c0b-2f8d-4fe6-ae55-fea341da535e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f650b09a-3d46-4623-9ffc-e7b7b58c57e4",
   "metadata": {},
   "source": [
    "### 테스트 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fd0b8229-7d1d-4ddf-9d52-a79004db5265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC: 오늘 날씨가 정말 좋네요.\n",
      "PRED: it's really a lot of the\n",
      "\n",
      "SRC: 이 모델이 제대로 번역을 할까요?\n",
      "PRED: this is the the the the ?\n",
      "\n",
      "SRC: 한국어를 영어로 번역하는 예제입니다.\n",
      "PRED: the translated translated translated translated .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"오늘 날씨가 정말 좋네요.\",\n",
    "    \"이 모델이 제대로 번역을 할까요?\",\n",
    "    \"한국어를 영어로 번역하는 예제입니다.\"\n",
    "]\n",
    "\n",
    "for s in test_sentences:\n",
    "    print(\"SRC:\", s)\n",
    "    print(\"PRED:\", translate_ko2en(model, s))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784bf4a5-8eb6-4375-9c25-c92db3528d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec033b-d34a-4cc6-aaa9-f573042e87bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
